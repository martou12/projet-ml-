{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8453b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "proj_root = pathlib.Path(\"..\").resolve()\n",
    "if str(proj_root) not in sys.path:\n",
    "    sys.path.insert(0, str(proj_root))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f1dd843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_feat shape: (9267, 22)\n",
      "Période: 1990-01-03 → 2025-10-23\n",
      "Train size: 7413 | Test size: 1854\n",
      "Target balance: y_up\n",
      "1    0.502\n",
      "0    0.498\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>ret_lag1</th>\n",
       "      <th>ret_lag2</th>\n",
       "      <th>ret_lag3</th>\n",
       "      <th>ret_lag5</th>\n",
       "      <th>ret_rollmean_5</th>\n",
       "      <th>ret_rollstd_5</th>\n",
       "      <th>ret_rollstd_10</th>\n",
       "      <th>ret_rollstd_20</th>\n",
       "      <th>abs_ret_lag1</th>\n",
       "      <th>range_pct</th>\n",
       "      <th>vix_lag1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9262</th>\n",
       "      <td>2025-10-17</td>\n",
       "      <td>1.16708</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>0.003084</td>\n",
       "      <td>0.003128</td>\n",
       "      <td>0.004107</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.003299</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>0.006084</td>\n",
       "      <td>25.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9263</th>\n",
       "      <td>2025-10-20</td>\n",
       "      <td>1.16449</td>\n",
       "      <td>-0.001514</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>0.003084</td>\n",
       "      <td>-0.003513</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.002880</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.003519</td>\n",
       "      <td>0.001514</td>\n",
       "      <td>0.003143</td>\n",
       "      <td>20.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9264</th>\n",
       "      <td>2025-10-21</td>\n",
       "      <td>1.16050</td>\n",
       "      <td>-0.002219</td>\n",
       "      <td>-0.001514</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>0.003128</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>0.003281</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.004929</td>\n",
       "      <td>18.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9265</th>\n",
       "      <td>2025-10-22</td>\n",
       "      <td>1.16128</td>\n",
       "      <td>-0.003426</td>\n",
       "      <td>-0.002219</td>\n",
       "      <td>-0.001514</td>\n",
       "      <td>0.003084</td>\n",
       "      <td>-0.000533</td>\n",
       "      <td>0.002855</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>0.003312</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>0.003875</td>\n",
       "      <td>17.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9266</th>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>1.15958</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>-0.003426</td>\n",
       "      <td>-0.002219</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>-0.001590</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.003054</td>\n",
       "      <td>0.002995</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.002052</td>\n",
       "      <td>18.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date    close  ret_lag1  ret_lag2  ret_lag3  ret_lag5  \\\n",
       "9262 2025-10-17  1.16708  0.003822  0.003084  0.003128  0.004107   \n",
       "9263 2025-10-20  1.16449 -0.001514  0.003822  0.003084 -0.003513   \n",
       "9264 2025-10-21  1.16050 -0.002219 -0.001514  0.003822  0.003128   \n",
       "9265 2025-10-22  1.16128 -0.003426 -0.002219 -0.001514  0.003084   \n",
       "9266 2025-10-23  1.15958  0.000672 -0.003426 -0.002219  0.003822   \n",
       "\n",
       "      ret_rollmean_5  ret_rollstd_5  ret_rollstd_10  ret_rollstd_20  \\\n",
       "9262        0.001001       0.003299        0.003726        0.003673   \n",
       "9263        0.001260       0.002880        0.003700        0.003519   \n",
       "9264       -0.000051       0.003281        0.003593        0.003541   \n",
       "9265       -0.000533       0.002855        0.003535        0.003312   \n",
       "9266       -0.001590       0.001492        0.003054        0.002995   \n",
       "\n",
       "      abs_ret_lag1  range_pct  vix_lag1  \n",
       "9262      0.003822   0.006084     25.31  \n",
       "9263      0.001514   0.003143     20.78  \n",
       "9264      0.002219   0.004929     18.23  \n",
       "9265      0.003426   0.003875     17.87  \n",
       "9266      0.000672   0.002052     18.60  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.dataset import build_dataset\n",
    "from src.modeling import train_test_split_time\n",
    "from src.config import TARGET_COL, FEATURE_COLS\n",
    "\n",
    "df_feat = build_dataset(include_vix=True).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split_time(df_feat, test_size=0.2)\n",
    "\n",
    "print(\"df_feat shape:\", df_feat.shape)\n",
    "print(\"Période:\", df_feat[\"date\"].min().date(), \"→\", df_feat[\"date\"].max().date())\n",
    "print(\"Train size:\", len(X_train), \"| Test size:\", len(X_test))\n",
    "print(\"Target balance:\", df_feat[TARGET_COL].value_counts(normalize=True).round(3))\n",
    "\n",
    "df_feat[[\"date\", \"close\"] + [c for c in FEATURE_COLS if c in df_feat.columns]].tail(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed835b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== XGBoost (advanced) ===\n",
      "Accuracy: 0.504\n",
      "ROC-AUC : 0.506\n",
      "Confusion matrix:\n",
      " [[425 498]\n",
      " [421 510]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.502     0.460     0.480       923\n",
      "           1      0.506     0.548     0.526       931\n",
      "\n",
      "    accuracy                          0.504      1854\n",
      "   macro avg      0.504     0.504     0.503      1854\n",
      "weighted avg      0.504     0.504     0.503      1854\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5043149946062567, 0.505595167302252)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.evaluation import evaluate_classifier\n",
    "import xgboost as xgb\n",
    "# Pourquoi XGBoost:\n",
    "# - très bon modèle tabulaire (arbres boostés)\n",
    "# - capture des non-linéarités (utile en finance)\n",
    "# - reste \"classique ML\", pas deep learning\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "evaluate_classifier(xgb_model, X_train, y_train, X_test, y_test, name=\"XGBoost (advanced)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d65193c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brier XGB uncal: 0.256\n",
      "Brier XGB cal  : 0.2503\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "# calibration \"sigmoid\" = Platt scaling (simple, marche souvent)\n",
    "xgb_cal = CalibratedClassifierCV(xgb_model, method=\"sigmoid\", cv=3)\n",
    "xgb_cal.fit(X_train, y_train)\n",
    "\n",
    "proba_uncal = xgb_model.predict_proba(X_test)[:, 1]\n",
    "proba_cal = xgb_cal.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Brier XGB uncal:\", round(brier_score_loss(y_test, proba_uncal), 4))\n",
    "print(\"Brier XGB cal  :\", round(brier_score_loss(y_test, proba_cal), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "615bc40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LogReg (advanced config) ===\n",
      "Accuracy: 0.504\n",
      "ROC-AUC : 0.496\n",
      "Confusion matrix:\n",
      " [[334 589]\n",
      " [331 600]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.502     0.362     0.421       923\n",
      "           1      0.505     0.644     0.566       931\n",
      "\n",
      "    accuracy                          0.504      1854\n",
      "   macro avg      0.503     0.503     0.493      1854\n",
      "weighted avg      0.503     0.504     0.494      1854\n",
      "\n",
      "=== RandomForest (advanced config) ===\n",
      "Accuracy: 0.497\n",
      "ROC-AUC : 0.499\n",
      "Confusion matrix:\n",
      " [[313 610]\n",
      " [322 609]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.493     0.339     0.402       923\n",
      "           1      0.500     0.654     0.567       931\n",
      "\n",
      "    accuracy                          0.497      1854\n",
      "   macro avg      0.496     0.497     0.484      1854\n",
      "weighted avg      0.496     0.497     0.485      1854\n",
      "\n",
      "=== GradientBoosting (advanced config) ===\n",
      "Accuracy: 0.511\n",
      "ROC-AUC : 0.52\n",
      "Confusion matrix:\n",
      " [[399 524]\n",
      " [382 549]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.511     0.432     0.468       923\n",
      "           1      0.512     0.590     0.548       931\n",
      "\n",
      "    accuracy                          0.511      1854\n",
      "   macro avg      0.511     0.511     0.508      1854\n",
      "weighted avg      0.511     0.511     0.508      1854\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.511326860841424, 0.5198978719046494)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.modeling import build_logreg_pipeline, build_random_forest, build_gradient_boosting\n",
    "\n",
    "# Modèles \"cours\" + hyperparams déjà un peu réglés dans modeling.py\n",
    "lr = build_logreg_pipeline()\n",
    "rf = build_random_forest()\n",
    "gb = build_gradient_boosting()\n",
    "\n",
    "for m in [lr, rf, gb]:\n",
    "    m.fit(X_train, y_train)\n",
    "\n",
    "evaluate_classifier(lr, X_train, y_train, X_test, y_test, name=\"LogReg (advanced config)\")\n",
    "evaluate_classifier(rf, X_train, y_train, X_test, y_test, name=\"RandomForest (advanced config)\")\n",
    "evaluate_classifier(gb, X_train, y_train, X_test, y_test, name=\"GradientBoosting (advanced config)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2376d30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Manual stacking ===\n",
      "Accuracy: 0.496\n",
      "ROC-AUC : 0.489\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Stacking manuel : on crée des probas OOF sur le train, puis une logreg combine tout ça\n",
    "\n",
    "base_models = {\n",
    "    \"lr\": build_logreg_pipeline(),\n",
    "    \"rf\": build_random_forest(),\n",
    "    \"gb\": build_gradient_boosting(),\n",
    "    \"xgb\": xgb.XGBClassifier(\n",
    "        n_estimators=400, max_depth=4, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        objective=\"binary:logistic\", eval_metric=\"logloss\",\n",
    "        random_state=0,\n",
    "    ),\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "oof = np.full((len(X_train), len(base_models)), np.nan)\n",
    "\n",
    "# 1) OOF probas sur TRAIN (walk-forward)\n",
    "for tr_idx, va_idx in tscv.split(X_train):\n",
    "    for j, model in enumerate(base_models.values()):\n",
    "        m = clone(model)\n",
    "        m.fit(X_train[tr_idx], y_train[tr_idx])\n",
    "        oof[va_idx, j] = m.predict_proba(X_train[va_idx])[:, 1]\n",
    "\n",
    "mask = ~np.isnan(oof).any(axis=1)\n",
    "X_meta_train, y_meta_train = oof[mask], y_train[mask]\n",
    "\n",
    "# 2) Meta-modèle = logreg sur les probas\n",
    "meta = LogisticRegression(max_iter=1000)\n",
    "meta.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# 3) Refit des modèles de base sur tout le train\n",
    "fitted_base = {name: clone(m).fit(X_train, y_train) for name, m in base_models.items()}\n",
    "\n",
    "# 4) Proba stacking (fonction utile pour la suite)\n",
    "def stack_manual_predict_proba(X):\n",
    "    base_probas = np.column_stack([m.predict_proba(X)[:, 1] for m in fitted_base.values()])\n",
    "    return meta.predict_proba(base_probas)[:, 1]\n",
    "\n",
    "# 5) Eval rapide sur TEST\n",
    "proba_stack = stack_manual_predict_proba(X_test)\n",
    "pred_stack = (proba_stack >= 0.5).astype(int)\n",
    "\n",
    "print(\"=== Manual stacking ===\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, pred_stack), 3))\n",
    "print(\"ROC-AUC :\", round(roc_auc_score(y_test, proba_stack), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "639371aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Voting ensemble (soft) ===\n",
      "Accuracy: 0.497\n",
      "ROC-AUC : 0.508\n",
      "Confusion matrix:\n",
      " [[378 545]\n",
      " [388 543]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.493     0.410     0.448       923\n",
      "           1      0.499     0.583     0.538       931\n",
      "\n",
      "    accuracy                          0.497      1854\n",
      "   macro avg      0.496     0.496     0.493      1854\n",
      "weighted avg      0.496     0.497     0.493      1854\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4967637540453074, 0.5081000752927047)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Voting \"soft\" = moyenne des proba (mais pas d'apprentissage des poids)\n",
    "voting = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"lr\", build_logreg_pipeline()),\n",
    "        (\"rf\", build_random_forest()),\n",
    "        (\"gb\", build_gradient_boosting()),\n",
    "        (\"xgb\", xgb_model),\n",
    "    ],\n",
    "    voting=\"soft\",\n",
    ")\n",
    "\n",
    "voting.fit(X_train, y_train)\n",
    "evaluate_classifier(voting, X_train, y_train, X_test, y_test, name=\"Voting ensemble (soft)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea7fe5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold (OOF train): 0.4 | OOF Train F1: 0.668\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Ici on utilise les probas OOF du meta-modèle (pas de fuite)\n",
    "# X_meta_train = probas des modèles de base (OOF)\n",
    "# meta = le meta-modèle déjà fit\n",
    "\n",
    "proba_meta_oof = meta.predict_proba(X_meta_train)[:, 1]\n",
    "\n",
    "def best_threshold_by_f1_from_proba(y_true, proba):\n",
    "    thresholds = np.linspace(0.40, 0.60, 81)  # zone réaliste\n",
    "    best_t, best_f1 = 0.5, -1\n",
    "    for t in thresholds:\n",
    "        pred = (proba >= t).astype(int)\n",
    "        score = f1_score(y_true, pred)\n",
    "        if score > best_f1:\n",
    "            best_f1 = score\n",
    "            best_t = t\n",
    "    return float(best_t), float(best_f1)\n",
    "\n",
    "best_t, best_f1 = best_threshold_by_f1_from_proba(y_meta_train, proba_meta_oof)\n",
    "print(\"Best threshold (OOF train):\", round(best_t, 3), \"| OOF Train F1:\", round(best_f1, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "904af298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Manual Stacking (threshold tuned on OOF train) ===\n",
      "Threshold: 0.4\n",
      "Accuracy: 0.502\n",
      "ROC-AUC : 0.489\n",
      "Confusion matrix:\n",
      " [[  0 923]\n",
      " [  0 931]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000       923\n",
      "           1      0.502     1.000     0.669       931\n",
      "\n",
      "    accuracy                          0.502      1854\n",
      "   macro avg      0.251     0.500     0.334      1854\n",
      "weighted avg      0.252     0.502     0.336      1854\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simon\\OneDrive\\Bureau\\Projet Machine Learning\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\simon\\OneDrive\\Bureau\\Projet Machine Learning\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\simon\\OneDrive\\Bureau\\Projet Machine Learning\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# proba_stack vient de la cellule \"Manual Stacking\"\n",
    "pred_test = (proba_stack >= best_t).astype(int)\n",
    "\n",
    "print(\"=== Manual Stacking (threshold tuned on OOF train) ===\")\n",
    "print(\"Threshold:\", round(best_t, 3))\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, pred_test), 3))\n",
    "print(\"ROC-AUC :\", round(roc_auc_score(y_test, proba_stack), 3))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, pred_test))\n",
    "print(\"\\nReport:\\n\", classification_report(y_test, pred_test, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59e201a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Single-date prediction (random TEST date) ===\n",
      "Date: 2019-04-12 00:00:00\n",
      "EUR/USD close: 1.1297\n",
      "VIX: 12.01\n",
      "\n",
      "--- Avg(models) ---\n",
      "Probabilités UP par modèle: {'LogReg': 0.4950935477258546, 'RandomForest': 0.5191494412361286, 'GradientBoosting': 0.5417275052143741, 'XGBoost': 0.5474579930305481}\n",
      "Moyenne P(UP): 0.526\n",
      "Prédiction J+1: UP\n",
      "\n",
      "--- Manual stacking ---\n",
      "P(UP) stacking: 0.496\n",
      "Prédiction J+1 (stacking): DOWN\n",
      "\n",
      "Vérité terrain y_up: 1\n"
     ]
    }
   ],
   "source": [
    "from src.inference import predict_date_ensemble\n",
    "\n",
    "# Modèles déjà fit plus haut (lr/rf/gb + xgb_model)\n",
    "models_dict = {\n",
    "    \"LogReg\": lr,\n",
    "    \"RandomForest\": rf,\n",
    "    \"GradientBoosting\": gb,\n",
    "    \"XGBoost\": xgb_model,\n",
    "}\n",
    "\n",
    "# 1) moyenne des probas (comme ton inference.py)\n",
    "res = predict_date_ensemble(\n",
    "    df_feat=df_feat,\n",
    "    models=models_dict,\n",
    "    date=None,          # None = random dans zone test\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2) stacking manuel (proba calculée via stack_manual_predict_proba)\n",
    "# on récupère les features de la même date\n",
    "row = df_feat[df_feat[\"date\"] == res[\"date\"]].iloc[0]\n",
    "X_row = row[[c for c in FEATURE_COLS if c in row.index]].values.reshape(1, -1)\n",
    "p_stack = float(stack_manual_predict_proba(X_row)[0])\n",
    "\n",
    "print(\"=== Single-date prediction (random TEST date) ===\")\n",
    "print(\"Date:\", res[\"date\"])\n",
    "print(\"EUR/USD close:\", float(row[\"close\"]))\n",
    "print(\"VIX:\", float(row[\"vix\"]) if (\"vix\" in row.index and pd.notna(row[\"vix\"])) else \"NA\")\n",
    "\n",
    "print(\"\\n--- Avg(models) ---\")\n",
    "print(\"Probabilités UP par modèle:\", res[\"model_probas_up\"])\n",
    "print(\"Moyenne P(UP):\", round(res[\"avg_proba_up\"], 3))\n",
    "print(\"Prédiction J+1:\", res[\"predicted_direction\"])\n",
    "\n",
    "print(\"\\n--- Manual stacking ---\")\n",
    "print(\"P(UP) stacking:\", round(p_stack, 3))\n",
    "print(\"Prédiction J+1 (stacking):\", \"UP\" if p_stack >= 0.5 else \"DOWN\")\n",
    "\n",
    "print(\"\\nVérité terrain y_up:\", int(row[TARGET_COL]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e0ea66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0c2a7419144fcdbaae749aebc2dde1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(ToggleButtons(description='Scope:', options=(('Test (20% fin)', 'test'), ('Toutes dates', 'all'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95230d5e8b4c48489343000d24c8231b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Dataset déjà prêt\n",
    "df_feat_ui = df_feat.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# Définir zone test = dernier 20%\n",
    "n = len(df_feat_ui)\n",
    "cut = int(n * 0.8)\n",
    "\n",
    "all_dates = df_feat_ui[\"date\"].dt.strftime(\"%Y-%m-%d\").tolist()\n",
    "test_dates = df_feat_ui.iloc[cut:][\"date\"].dt.strftime(\"%Y-%m-%d\").tolist()\n",
    "\n",
    "# UI\n",
    "scope_toggle = widgets.ToggleButtons(\n",
    "    options=[(\"Test (20% fin)\", \"test\"), (\"Toutes dates\", \"all\")],\n",
    "    value=\"test\",\n",
    "    description=\"Scope:\"\n",
    ")\n",
    "\n",
    "date_dd = widgets.Dropdown(\n",
    "    options=test_dates if len(test_dates) > 0 else all_dates,\n",
    "    description=\"Date:\",\n",
    "    layout=widgets.Layout(width=\"260px\")\n",
    ")\n",
    "\n",
    "btn = widgets.Button(description=\"Prédire\")\n",
    "out = widgets.Output()\n",
    "\n",
    "def update_dates(change=None):\n",
    "    opts = test_dates if scope_toggle.value == \"test\" else all_dates\n",
    "    date_dd.options = opts\n",
    "    if len(opts) > 0:\n",
    "        date_dd.value = opts[0]\n",
    "\n",
    "scope_toggle.observe(update_dates, names=\"value\")\n",
    "\n",
    "def predict_for_date(date_str):\n",
    "    date = pd.to_datetime(date_str)\n",
    "\n",
    "    row_df = df_feat_ui[df_feat_ui[\"date\"] == date]\n",
    "    if row_df.empty:\n",
    "        raise ValueError(\"Date introuvable dans df_feat.\")\n",
    "    row = row_df.iloc[0]\n",
    "\n",
    "    # valeurs à afficher\n",
    "    eur_close = float(row[\"close\"])\n",
    "    vix_val = float(row[\"vix\"]) if (\"vix\" in row.index and pd.notna(row[\"vix\"])) else None\n",
    "    actual = int(row[TARGET_COL])\n",
    "\n",
    "    # features pour modèles\n",
    "    feat_cols = [c for c in FEATURE_COLS if c in row.index]\n",
    "    X_row = row[feat_cols].values.reshape(1, -1)\n",
    "\n",
    "    # probas par modèle + moyenne\n",
    "    probas_models = {name: float(m.predict_proba(X_row)[0, 1]) for name, m in models_dict.items()}\n",
    "    p_avg = float(np.mean(list(probas_models.values())))\n",
    "\n",
    "    # proba stacking manuel\n",
    "    p_stack = float(stack_manual_predict_proba(X_row)[0])\n",
    "\n",
    "    return eur_close, vix_val, probas_models, p_avg, p_stack, actual\n",
    "\n",
    "def run_pred(_):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        eur_close, vix_val, probas_models, p_avg, p_stack, actual = predict_for_date(date_dd.value)\n",
    "\n",
    "        print(\"Date:\", date_dd.value)\n",
    "        print(\"EUR/USD close:\", eur_close)\n",
    "        print(\"VIX:\", vix_val if vix_val is not None else \"NA\")\n",
    "\n",
    "        print(\"\\nProbabilités UP par modèle:\")\n",
    "        for k, v in probas_models.items():\n",
    "            print(\"-\", k, \":\", round(v, 3))\n",
    "\n",
    "        print(\"\\nMoyenne P(UP):\", round(p_avg, 3), \"=>\", \"UP\" if p_avg >= 0.5 else \"DOWN\")\n",
    "        print(\"Stacking P(UP):\", round(p_stack, 3), \"=>\", \"UP\" if p_stack >= 0.5 else \"DOWN\")\n",
    "        print(\"\\nVérité terrain y_up:\", actual)\n",
    "\n",
    "btn.on_click(run_pred)\n",
    "\n",
    "display(widgets.HBox([scope_toggle, date_dd, btn]))\n",
    "display(out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
